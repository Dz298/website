<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Dingqi Zhang</title>

  <meta name="author" content="Dingqi Zhang">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <!-- <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>"> -->
  <link rel="icon" type="image/x-icon" href="images/DALLE.ico">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Dingqi(Daisy) Zhang</name>
                  </p>
                  <p>
                    I am a PhD candidate at UC Berkeley advised by <a
                      href="https://scholar.google.com/citations?user=yQxs7qUAAAAJ&hl=en">Prof. Mark W. Mueller</a> and
                    <a href="https://people.eecs.berkeley.edu/~malik/">Prof. Jitendra Malik</a> from the
                    <a href="https://bair.berkeley.edu">Berkeley AI Research Lab</a> (BAIR).
                    I am interested in bringing adaptation and agility to robots with an inspiration from cognitive
                    science by deep learning methods.
                  </p>
                  Before coming to Berkeley, I finished my undergrad at Cornell University,
                  with a double major in Computer Science and Mechanical Engineering. I have also visited <a
                    href="https://www4.mae.cuhk.edu.hk/peoples/chen-benmei/">
                    Prof. Ben M. Chen</a>'s lab at the Chinese University of Hong Kong during summer, 2024.
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:dingqi@berkeley.edu">Email</a> &nbsp/&nbsp

                    <a href="file/Dingqi_Zhang_Resume-Oct-24.pdf">Resume</a> &nbsp/&nbsp
                    <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp/&nbsp -->
                    <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                    <a href="https://scholar.google.com/citations?user=rzknuKEAAAAJ&hl=en">Google Scholar</a>
                    &nbsp/&nbsp
                    <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp/&nbsp -->
                    <a href="https://www.linkedin.com/in/dingqi-zhang/">LinkedIn</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <img style="width:100%;max-width:100%" alt="profile photo" src="images/profile.jpeg"
                    class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>News </heading>
                  <ul>
                    <li>Our paper is spotlighted on <a
                        href="https://spectrum.ieee.org/video-friday-quadruped-ladder-climbing">IEEE Spectrum Video
                        Friday</a> (Oct 4th, 2024) as the only drone in all robots!
                    </li>
                    <br>
                    <li>I was invited to give a talk:
                      A Data-Driven Adaptive Controller for Extreme Parameter Variance<br> <i>Hong Kong Polytechnic
                        University @ IPNL</i> (July, 2024)
                    </li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research</heading>
                  <!-- <p>
                I'm interested in computer vision, machine learning, optimization, and image processing. Much of my research is about inferring the physical world (shape, motion, color, light, etc) from images. Representative papers are <span class="highlight">highlighted</span>.
              </p> -->
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr></tr>
              <td width="37%" valign="top" align="center">
                <img src="papers/TRO_VideoSupp-ezgif.com-cut.gif" alt="sym" width="90%"
                  style="margin-top:15px;padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black">
              </td>
              <td width="60%" valign="top">
                <p><a href="https://github.com/muellerlab/xadapt_ctrl" id="TRO-xadap">
                    <papertitle>A Learning-based Quadcopter Controller with Extreme Adaptation
                    </papertitle>
                  </a><br>
                  <b>Dingqi Zhang</b>, Antonio Loquercio, Jerry Tang, Ting-Hao Wang, <br />Jitendra Malik, Mark W.
                  Mueller<br>
                  <i>Preprint</i>, 2024
                </p>

                <div class="paper" id="xadap">
                  <a href="https://github.com/muellerlab/xadapt_ctrl">code</a> |
                  <a href="javascript:toggleblock('xadap_abs')">abstract</a> |
                  <a shape="rect" href="javascript:togglebib('xadap_bib')" class="togglebib">bibtex</a> |
                  <a href="https://arxiv.org/abs/2409.12949">arXiv</a> |
                  <a href="https://youtu.be/kZEU8lxMZug">video</a>

                  <p align="justify"> <i id="xadap_abs">This paper introduces a learning-based low-level controller for
                      quadcopters, which adaptively controls quadcopters with significant variations in mass, size, and
                      actuator capabilities. Our approach leverages a combination of imitation learning and
                      reinforcement learning, creating a fast-adapting and general control framework for quadcopters
                      that eliminates the need for precise model estimation or manual tuning. The controller estimates a
                      latent representation of the vehicle's system parameters from sensor-action history, enabling it
                      to adapt swiftly to diverse dynamics.
                      Extensive evaluations in simulation demonstrate the controller's ability to generalize to unseen
                      quadcopter parameters, with an adaptation range up to 16 times broader than the training set. In
                      real-world tests, the controller is successfully deployed on quadcopters with mass differences of
                      3.7 times and propeller constants varying by more than 100 times, while also showing rapid
                      adaptation to disturbances such as off-center payloads and motor failures. These results highlight
                      the potential of our controller in extreme adaptation to simplify the design process and enhance
                      the reliability of autonomous drone operations in unpredictable environments.</i></p>
                  <small><i id="xadap_bib">
                      <pre xml:space="preserve">
@misc{zhang2024learningbasedquadcoptercontrollerextreme,
  title={A Learning-based Quadcopter Controller 
    with Extreme Adaptation}, 
  author={Dingqi Zhang and Antonio Loquercio 
    and Jerry Tang and Ting-Hao Wang 
    and Jitendra Malik and Mark W. Mueller},
  year={2024},
  eprint={2409.12949},
  archivePrefix={arXiv},
  primaryClass={cs.RO},
  url={https://arxiv.org/abs/2409.12949}, 
}

</pre>
                    </i></small>
                </div>
              </td>
      </tr>

      <tr></tr>
      <td width="37%" valign="top" align="center">
        <video autoplay loop muted src="papers/proxfly.mov" alt="sym" width="90%"
          style="margin-top:15px;padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black"></video>
      </td>
      <td width="60%" valign="top">
        <p>
          <papertitle>ProxFly: Robust Control for Close Proximity Quadcopter Flight via Residual Reinforcement
            Learning
          </papertitle>
          <br>
          Ruiqi Zhang, <b>Dingqi Zhang</b>, Mark W. Mueller<br>
          <i>Preprint</i>, 2024
        </p>

        <div class="paper" id="proxfly">
          <a href="javascript:toggleblock('proxfly_abs')">abstract</a> | <!--TODO-->
          <a shape="rect" href="javascript:togglebib('proxfly_bib')" class="togglebib">bibtex</a> | <!--TODO-->
          <a href="https://arxiv.org/abs/2409.13193">arXiv</a> | <!--TODO-->
          <a href="https://youtu.be/NhPKgzd3l6w?si=tRCYwWsWDxtwjfrc">video</a> <!--TODO-->

          <p align="justify"> <i id="proxfly_abs">This paper proposes the ProxFly, a residual deep
              Reinforcement Learning (RL)-based controller for close prox-
              imity quadcopter flight. Specifically, we design a residual mod-
              ule on top of a cascaded controller (denoted as basic controller)
              to generate high-level control commands, which compensate
              for external disturbances and thrust loss caused by downwash
              effects from other quadcopters. First, our method takes only the
              ego state and controllers‚Äô commands as inputs and does not rely
              on any communication between quadcopters, thereby reducing
              the bandwidth requirement. Through domain randomization,
              our method relaxes the requirement for accurate system iden-
              tification and fine-tuned controller parameters, allowing it to
              adapt to changing system models. Meanwhile, our method
              not only reduces the proportion of unexplainable signals from
              the black box in control commands but also enables the RL
              training to skip the time-consuming exploration from scratch
              via guidance from the basic controller. We validate the effec-
              tiveness of the residual module in the simulation with different
              proximities. Moreover, we conduct the real close proximity
              flight test to compare ProxFly with the basic controller and
              an advanced model-based controller with complex aerodynamic
              compensation. Finally, we show that ProxFly can be used for
              challenging quadcopter in-air docking, where two quadcopters
              fly in extreme proximity, and strong airflow significantly dis-
              rupts flight. However, our method can stabilize the quadcopter
              in this case and accomplish docking.</i></p>
          <small><i id="proxfly_bib">
              <pre xml:space="preserve">
@misc{zhang2024proxflyrobustcontrolclose,
  title={ProxFly: Robust Control for Close Proximity 
    Quadcopter Flight via Residual Reinforcement Learning}, 
  author={Ruiqi Zhang and Dingqi Zhang and Mark W. Mueller},
  year={2024},
  eprint={2409.13193},
  archivePrefix={arXiv},
  primaryClass={cs.RO},
  url={https://arxiv.org/abs/2409.13193}, 
}

</pre>
            </i></small>
        </div>
      </td>
      </tr>

      <tr>
        <td width="37%" valign="top" align="center"><a href="https://dz298.github.io/universal-drone-controller/">
            <video autoplay loop muted src="images/trimm-icra23-v7-2.mp4" alt="sym" width="90%"
              style="margin-top:15px;padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black"></video>
          </a></td>
        <td width="60%" valign="top">
          <p><a href="https://dz298.github.io/universal-drone-controller/" id="universal-drone-controller">
              <papertitle>Learning a Single Near-hover Position Controller for Vastly Different Quadcopters
              </papertitle>
            </a><br>
            <b>Dingqi Zhang</b>, Antonio Loquercio, Xiangyu Wu, Ashish Kumar, <br />Jitendra Malik, Mark W.
            Mueller<br>
            <i>International Conference on Robotics and Automation (ICRA)</i>, 2023
          </p>

          <div class="paper" id="udc">
            <a href="https://dz298.github.io/universal-drone-controller/">webpage</a> |
            <a href="https://github.com/Dz298/adap_drone_lowlevelctrl">code</a> |
            <a href="javascript:toggleblock('udc_abs')">abstract</a> |
            <a shape="rect" href="javascript:togglebib('udc_bib')" class="togglebib">bibtex</a> |
            <a href="https://arxiv.org/abs/2209.09232">arXiv</a> |
            <a href="https://youtu.be/U-c-LbTfvoA">video</a>

            <p align="justify"> <i id="udc_abs">This paper proposes an adaptive near-hover position controller
                for quadcopters, which can be deployed to quadcopters of very different mass, size and motor
                constants, and also shows rapid adaptation to unknown disturbances during runtime. The core
                algorithmic idea is to learn a single policy that can adapt online at test time not only to the
                disturbances applied to the drone, but also to the robot dynamics and hardware in the same
                framework. We achieve this by training a neural network to estimate a latent representation of
                the robot and environment parameters, which is used to condition the behaviour of the
                controller, also represented as a neural network. We train both networks exclusively in
                simulation with the goal of flying the quadcopters to goal positions and avoiding crashes to the
                ground. We directly deploy the same controller trained in the simulation without any
                modifications on two quadcopters in the real world with differences in mass, size, motors, and
                propellers with mass differing by 4.5 times. In addition, we show rapid adaptation to sudden and
                large disturbances up to one-third of the mass of the quadcopters. We perform an extensive
                evaluation in both simulation and the physical world, where we outperform a state-of-the-art
                learning-based adaptive controller and a traditional PID controller specifically tuned to each
                platform individually.</i></p>
            <small><i id="udc_bib">
                <pre xml:space="preserve">
@article{zhang2023learning,
  title={Learning a Single Near-hover Position Controller 
    for Vastly Different Quadcopters},
  author={Zhang, Dingqi and Loquercio, Antonio and 
  Wu, Xiangyu and Kumar, Ashish and 
  Malik, Jitendra and Mueller, Mark W},
  journal={arXiv preprint arXiv:2209.09232},
  year={2022}
}

</pre>
              </i></small>
          </div>
        </td>
      </tr>

    </tbody>
  </table>


  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>Project</heading>
        </td>
      </tr>
    </tbody>
  </table>
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>

      <tr>
        <td width="37%" valign="top" align="center"><a>
            <video autoplay loop muted src="images/tennie_on_court.mov" alt="sym" width="90%"
              style="margin-top:15px;padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black"></video>
          </a></td>
        <td width="60%" valign="top">
          <p><a id="tennibot">
              <papertitle>Tennie: An Antonomous Tennis Ball Collector</papertitle>
            </a><br>
            <b>Dingqi Zhang*</b>, Jerry Tang*<br>
            <span style="color: rgb(247, 101, 4);">(SP24 Ignite Grant, 3% | FA23 Spark Grant, 9%)</span>
          </p>

          <div class="paper" id="tb">
            <a href="javascript:toggleblock('tb_abs')">abstract</a> |
            <a href="images/tennie_collection.mov">video</a>
          </div>
          <p align="justify"> <i id="tb_abs">Ball collection has always been a laborous and time-consuming work
              in tennis training. We design Tennie to automate this process, so that players and coaches can
              dedicate their efforts entirely to skill development. Tennie is a mobile robot that can collect
              tennis balls autonomously. It is equipped with a roller to collect balls and a cassie platform to
              move freely. We have built a prototype and demonstrated its functionality. We are currently
              working on adding vision into the system so that the Tennie can compute optimal paths for
              maximum collection efficiency based on vision inputs.
              Our proposal has been selected for Ignite Grant in Spring 2024 and Spark Grant in Fall 2023 by <a
                href="https://jacobsinstitute.berkeley.edu/the-jacobs-institute-innovation-catalysts/">the
                Jacobs Institude Innovation Catalysts</a>.
          </p>

        </td>
      </tr>
    </tbody>
  </table>


  <table width="100%" align="center" border="0" cellpadding="20">
    <tbody>
      <tr>
        <td>
          <heading>Personal</heading> <br> <br>
          <heading1>
            Climbing : Red Rock Boulders, Nevada <br>
            Skiing : Palisades Tahoe, California <br>
            Tennis: Berkeley, California <br>
          </heading1>
        </td>
      </tr>
    </tbody>
  </table>
  <table width="100%" align="center" border="0" cellpadding="20">
    <tbody>
      <tr>
        <td style="padding:20px;width:30%;vertical-align:middle">
          <img style="height:200px;width:100%;object-fit:cover" src="images/climb.jpeg">
        </td>
        <td style="padding:20px;width:30%;vertical-align:middle">
          <img style="height:200px;width:100%;object-fit:cover" src="images/ski.jpg">
        </td>
        <td style="padding:20px;width:30%;vertical-align:middle">
          <img style="height:200px;width:100%;object-fit:cover" src="images/tennis.JPG">
        </td>
      </tr>
      <tr>
        <td colspan="3" style="text-align:center">
          <heading1>
            I'm also raising a <a href="javascript:toggleblock('image_cat')">cat</a> named Èù¢Êù° ("myan-tyow"), adopted
            from <a href="https://berkeleyhumane.org">Berkeley Humane</a>.
          </heading1>
        </td>
      </tr>
    </tbody>
  </table>

  <p id="image_cat" style="display:none;text-align:center">
    <img style="height:200px;width:auto;object-fit:cover" src="images/cat.jpg" alt="Cat">
  </p>

  <table width="100%" align="right" border="0" cellpadding="20">
    <tbody>
      <br>
      <br>
      <tr>
        <td style="text-align:center;font-size:15px">Website adapted from
          <a href="https://jonbarron.info/">Jon Barron</a> and
          <a href="https://ashish-kmr.github.io">Ashish Kumar</a>; Last updated: Nov 13, 2024<br>
          <br>
          <a href="https://hits.seeyoufarm.com">
            <img
              src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fdz298.github.io%2Fwebsite%2F&count_bg=%23BFF0FB&title_bg=%23BCBCBC&icon=&icon_color=%23000000&title=hits&edge_flat=false" />
          </a>
        </td>
      </tr>
    </tbody>
  </table>

</body>

<script xml:space="preserve" language="JavaScript">
  hideblock('udc_abs');
  hideblock('tb_abs');
  hideblock('xadap_abs');
  hideblock('proxfly_abs');
</script>

<script xml:space="preserve" language="JavaScript">
  hideallbibs();
</script>

</html>